# 사투리 번역 모델과 분류 모델을 저장한 디렉토리

## F1 Score
- 강원도 : 0.86
- 전라도 : 0.91
- 경상도 : 0.94
- 제주도 : 0.69
- 충청도 : 0.91

## Classifier Accuracy(exactly match)
- 0.82

## 사용 데이터
- AI 허브 한국어 방언 발화
- 네이버 국어 사전 - 방언

## 사용 모델
- SKT-AI/KoBART
- skt/kobert-base-v1


## 개요
초기에 트랜스포머 구조만을 이용하여 번역을 시도하였는데 거의 사용할 수 없는 수준의 결과가 나타났습니다.
따라서 gpt나 bert를 이용하기로 하였습니다.
gpt-2 를 사용하여 번역 task에 맞게 fine-tuning을 하였을때 같은 단어의 반복적인 출력이 생기는 문제가 발생하였습니다.
이를 gready-search는 현재 위치에 가장 높은 확률의 단어를 선택하여 출력하기 때문인데 이를 보완하기 위해서 beam-search를 이용하였습니다.
beam-search는 연속적인 단어의 확률을 곱하여 현재 위치에 들어갈 단어를 선택하기 때문에 이를 해결할 수 있는 기술이라 생각했습니다.
하지만 대부분의 경우에서 뒤에 나오는 단어를 생성할때 확률이 너무 낮아 생성을 중단하여 대체로 문장의 길이가 너무 짧아지고
단어 1개만 생성하는 문제가 발생하였습니다.

추가로 자료 조사를 하여 BART를 알게되고 번역에 맞는 트랜스포머 구조와 마스킹, 회문, 단어 치환 등 noise를 원래 상태로 복구하는 pretrain 방식이
사투리라는 noise를 표준어로 바꾸는 과정과 매우 유사하다고 생각되어 BART를 번역 TASK에 맞게 fine-tuning하기로 하였습니다.

SKT-AI에서 제공하는 BART를 번역 TASK에 맞게 학습시켰더니 gpt-2의 반복 단어 생성과 같은 문제가 발생하지 않았습니다.
하지만 학습 데이터에 편향된 번역 결과로 인해 모든 사투리를 번역하지는 못하였고 일부 맞춤법이나 띄어쓰기가 틀렸을때 정확도가 매우 떨어졌습니다.

classifier 같은 경우는 일반적인 multi-classification 으로 만들었습니다.
각 5개 지방의 category로 분류하여 다중 분류를 하였고 82%의 성능을 보여주었습니다.